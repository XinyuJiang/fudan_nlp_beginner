{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../task3/snli_1.0/snli_1.0_train.txt'\n",
    "test_file = '../task3/snli_1.0/snli_1.0_test.txt'\n",
    "dev_file = '../task3/snli_1.0/snli_1.0_dev.txt'\n",
    "train = pd.read_csv(train_file, sep = '\\t')\n",
    "test = pd.read_csv(test_file, sep = '\\t')\n",
    "dev = pd.read_csv(dev_file, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, Dense\n",
    "lsen_len = 14#left sentence平均值，数据集的readme中拿到\n",
    "rsen_len = 8#right sentence平均值，数据集的readme中拿到\n",
    "embedding_size = 300\n",
    "embedding_file = '../../data/wordvector/fasttext/wiki-news-300d-1M.vec'\n",
    "vocab_size = 63035\n",
    "lstm_dim = 256\n",
    "compare_dim = 128\n",
    "compare_dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91479 nan\n",
      "91480 nan\n",
      "91481 nan\n",
      "311124 nan\n",
      "311125 nan\n",
      "311126 nan\n",
      "词数： 63035\n",
      "length of lhs and rhs: ['A', 'woman', 'reading', 'a', 'book', 'titled', 'the', 'boy', 'scout', 'hand', 'book.'] ['Some', 'people', 'are', 'on', 'a', 'beach', 'all', 'dressed', 'up', 'in', 'nice', 'clothes.']\n"
     ]
    }
   ],
   "source": [
    "##########数据统计和噪声处理#############\n",
    "vocab_set = set()\n",
    "lhs = []\n",
    "rhs = []\n",
    "for k,v in enumerate(train.sentence1):\n",
    "    if type(v) == float:\n",
    "        print(k,v)\n",
    "        invalid_idx.append(k)\n",
    "    else:\n",
    "        v = v.split()\n",
    "        lhs.append(v)\n",
    "        for j in v:\n",
    "            vocab_set.add(j)\n",
    "invalid_idx = []\n",
    "for k,v in enumerate(train.sentence2):\n",
    "    if type(v) == float:\n",
    "        print(k,v)\n",
    "        invalid_idx.append(k)\n",
    "    else:\n",
    "        v = v.split()\n",
    "        rhs.append(v)\n",
    "        for j in v:\n",
    "            vocab_set.add(j)\n",
    "for k,v in enumerate(train.gold_label):\n",
    "    if v not in label_dict.keys():\n",
    "        invalid_idx.append(k)\n",
    "\n",
    "print(\"词数：\",len(vocab_set))\n",
    "lhs.sort()\n",
    "rhs.sort()\n",
    "print(\"length of lhs and rhs:\",len(lhs[int(0.5*len(lhs))]),len(rhs[int(0.5*len(rhs))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理输入格式\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x1train = []\n",
    "x2train = []\n",
    "ytrain = []\n",
    "for k,v in enumerate(train.sentence1):\n",
    "    if k not in invalid_idx:\n",
    "        x1train.append(v)\n",
    "for k,v in enumerate(train.sentence2):\n",
    "    if k not in invalid_idx:\n",
    "        x2train.append(v)\n",
    "for k,v in enumerate(train.gold_label):\n",
    "    if k not in invalid_idx:\n",
    "        ytrain.append(v)\n",
    "x1train = pd.Series(x1train)\n",
    "x2train = pd.Series(x2train)\n",
    "ytrain = pd.Series(ytrain)\n",
    "x1test = test.sentence1\n",
    "x2test = test.sentence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(x1train.values)\n",
    "x1_train = tk.texts_to_sequences(x1train)\n",
    "x2_train = tk.texts_to_sequences(x2train)\n",
    "x1_test = tk.texts_to_sequences(x1test)\n",
    "x2_test = tk.texts_to_sequences(x2test)\n",
    "x1_train = pad_sequences(x1_train, lsen_len)\n",
    "x2_train = pad_sequences(x2_train, rsen_len)\n",
    "x1_test = pad_sequences(x1_test, lsen_len)\n",
    "x2_test = pad_sequences(x2_test, rsen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\"neutral\":0,\"contradiction\":1,\"entailment\":2}\n",
    "y_train = []\n",
    "count = 0\n",
    "for i in ytrain:\n",
    "    y_train.append(label_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ESIM Model##############################################\n",
    "def build_model(embedding_matrix=None):\n",
    "    ############输入编码##############\n",
    "    q1_inputs = Input(name='q1',shape=(lsen_len,))\n",
    "    q2_inputs = Input(name='q2',shape=(rsen_len,))#这里两个输入的长度不一定要保持一致，因为我们attention的是词和词\n",
    "    if embedding_matrix is None:\n",
    "        embedding = Embedding(vocab_size, embedding_size)\n",
    "    else:\n",
    "        embedding = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)\n",
    "    bn = BatchNormalization(axis=2)\n",
    "    q1_embed = bn(embedding(q1_inputs))\n",
    "    q2_embed = bn(embedding(q2_inputs))\n",
    "    q1_encoded = Bidirectional(LSTM(lstm_dim//2, return_sequences=True))(q1_embed)\n",
    "    q2_encoded = Bidirectional(LSTM(lstm_dim//2, return_sequences=True))(q2_embed)\n",
    "    #print(\"q1,q2 shape after bilstm:\",q1_encode.shape,q2_encode.shape)#输出格式为(x1,x2,x3),x1是批数，x2是句子长度，x3是bilstm隐层变量\n",
    "    ############局部推理(attention)##############\n",
    "    def multiply(tensor):\n",
    "        input_1 = tensor[0]\n",
    "        input_2 = tensor[1]\n",
    "        #in1_aligned = K.zeros([input_1.shape[0], input_1.shape[2]], dtype=tf.float32)\n",
    "        for i in range(lsen_len):\n",
    "            attention_1 = Dot(axes=-1)([input_1[:,i,:],input_2])#(bs,8)\n",
    "            w_att_1 = Lambda(lambda x: keras.activations.softmax(x, axis=1),)(attention_1)#(bs,8)\n",
    "            vector = Dot(axes=1)([w_att_1,input_2])#(bsx8)和(bsx8x256)作点乘，得到(bsx256)\n",
    "            att_value = K.expand_dims(vector,axis=1)#把维度变成bsx1x256\n",
    "            if i == 0:\n",
    "                in1_aligned = att_value\n",
    "            else:\n",
    "                in1_aligned = tf.concat([in1_aligned,att_value],1)\n",
    "        #print(\"shape of my heart1:\",attention_1.shape,w_att_1.shape,vector.shape,att_value.shape,in1_aligned.shape)\n",
    "\n",
    "        #in2_aligned = K.zeros([input_1.shape[0], input_1.shape[2]], dtype=tf.float32)\n",
    "        for i in range(rsen_len):\n",
    "            attention_2 = Dot(axes=-1)([input_1,input_2[:,i,:]])#(bs,14)\n",
    "            w_att_2 = Lambda(lambda x: keras.activations.softmax(x, axis=1),)(attention_2)#(bs,14)\n",
    "            vector = Dot(axes=1)([w_att_2,input_1])#(bsx14)和(bsx14x256)作点乘，得到(bsx256)\n",
    "            att_value = K.expand_dims(vector,axis=1)#把维度变成bsx1x256\n",
    "            if i == 0:\n",
    "                in2_aligned = att_value\n",
    "            else:\n",
    "                in2_aligned = tf.concat([in2_aligned,att_value],1)\n",
    "        #print(\"shape of my heart2:\",attention_2.shape,w_att_2.shape,vector.shape,att_value.shape,in2_aligned.shape)\n",
    "        #print(type(in1_aligned),type(in2_aligned))\n",
    "\n",
    "        return [in1_aligned,in2_aligned]#这里如果不是return一个list会报错\n",
    "    \n",
    "    [in1_aligned, in2_aligned]=Lambda(multiply)([q1_encoded,q2_encoded])\n",
    "    ############推理合成##############\n",
    "    #print(\"shape:\",in1_aligned.shape,in2_aligned.shape)\n",
    "    # Compare\n",
    "    print(K.tf.multiply(q1_encoded, in1_aligned))\n",
    "    q1_combined = concatenate(\n",
    "        [q1_encoded, in1_aligned, subtract([q1_encoded, in1_aligned]), Lambda(lambda x:K.tf.multiply(x[0],x[1]))([q1_encoded, in1_aligned])])\n",
    "    q2_combined = concatenate(\n",
    "        [q2_encoded, in2_aligned, subtract([q2_encoded, in2_aligned]), Lambda(lambda x:K.tf.multiply(x[0],x[1]))([q2_encoded, in2_aligned])])\n",
    "    q1_dense1 = Dense(compare_dim, activation='relu')(q1_combined)\n",
    "    q2_dense1 = Dense(compare_dim, activation='relu')(q2_combined)\n",
    "    q1_compare = Dropout(compare_dropout)(q1_dense1)\n",
    "    q2_compare = Dropout(compare_dropout)(q2_dense1)\n",
    "#     dense2 = Dense(compare_dim, activation='relu')(drop1)\n",
    "#     drop2 = Dropout(compare_dropout)(dense2)\n",
    "    #q1_compare = drop2(q1_combined)\n",
    "    #q2_compare = drop2(q2_combined)\n",
    "\n",
    "    # Aggregate\n",
    "    q1_maxp = MaxPooling1D()(q1_compare)\n",
    "    q1_avgp = AveragePooling1D()(q1_compare)\n",
    "    q2_maxp = MaxPooling1D()(q2_compare)\n",
    "    q2_avgp = AveragePooling1D()(q2_compare)\n",
    "    res = Concatenate(1)([q1_maxp,q1_avgp,q2_maxp,q2_avgp])\n",
    "    flat = Flatten()(res)\n",
    "    dense3 = Dense(128, activation='relu')(flat)\n",
    "    preds = Dense(3, activation='softmax')(dense3)\n",
    "    \n",
    "    model = Model(inputs=[q1_inputs,q2_inputs], outputs=[preds])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul_3:0\", shape=(?, 14, 256), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "q1 (InputLayer)                 (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q2 (InputLayer)                 (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         multiple             18910500    q1[0][0]                         \n",
      "                                                                 q2[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor multiple             1200        embedding_4[0][0]                \n",
      "                                                                 embedding_4[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 14, 256)      439296      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 8, 256)       439296      batch_normalization_4[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_142 (Lambda)             [(None, 14, 256), (N 0           bidirectional_7[0][0]            \n",
      "                                                                 bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_7 (Subtract)           (None, 14, 256)      0           bidirectional_7[0][0]            \n",
      "                                                                 lambda_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_187 (Lambda)             (None, 14, 256)      0           bidirectional_7[0][0]            \n",
      "                                                                 lambda_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_8 (Subtract)           (None, 8, 256)       0           bidirectional_8[0][0]            \n",
      "                                                                 lambda_142[0][1]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_188 (Lambda)             (None, 8, 256)       0           bidirectional_8[0][0]            \n",
      "                                                                 lambda_142[0][1]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 14, 1024)     0           bidirectional_7[0][0]            \n",
      "                                                                 lambda_142[0][0]                 \n",
      "                                                                 subtract_7[0][0]                 \n",
      "                                                                 lambda_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 8, 1024)      0           bidirectional_8[0][0]            \n",
      "                                                                 lambda_142[0][1]                 \n",
      "                                                                 subtract_8[0][0]                 \n",
      "                                                                 lambda_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 14, 128)      131200      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 8, 128)       131200      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 14, 128)      0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 8, 128)       0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 7, 128)       0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_7 (AveragePoo (None, 7, 128)       0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 4, 128)       0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_8 (AveragePoo (None, 4, 128)       0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 22, 128)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 average_pooling1d_7[0][0]        \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "                                                                 average_pooling1d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2816)         0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 128)          360576      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 3)            387         dense_15[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 20,413,655\n",
      "Trainable params: 20,413,055\n",
      "Non-trainable params: 600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "310144/549361 [===============>..............] - ETA: 12:52 - loss: 0.7707 - acc: 0.6587"
     ]
    }
   ],
   "source": [
    "model.fit([x1_train,x2_train], y_train, batch_size=128, epochs=7, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
