{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Code_Tools\\Anaconda\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'E:\\\\study\\\\NLP seminar\\\\task3\\\\snli_1.0\\\\snli_1.0_train.txt'\n",
    "test_file = 'E:\\\\study\\\\NLP seminar\\\\task3\\snli_1.0\\\\snli_1.0_test.txt'\n",
    "dev_file = 'E:\\\\study\\\\NLP seminar\\\\task3\\\\snli_1.0\\\\snli_1.0_dev.txt'\n",
    "train = pd.read_csv(train_file, sep = '\\t')\n",
    "test = pd.read_csv(test_file, sep = '\\t')\n",
    "dev = pd.read_csv(dev_file, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, Dense\n",
    "lsen_len = 14#left sentence平均值，数据集的readme中拿到\n",
    "rsen_len = 8#right sentence平均值，数据集的readme中拿到\n",
    "embedding_size = 300\n",
    "embedding_file = '../../data/wordvector/fasttext/wiki-news-300d-1M.vec'\n",
    "vocab_size = 63035\n",
    "lstm_dim = 256\n",
    "compare_dim = 128\n",
    "compare_dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91479 nan\n",
      "91480 nan\n",
      "91481 nan\n",
      "311124 nan\n",
      "311125 nan\n",
      "311126 nan\n",
      "词数： 63035\n",
      "length of lhs and rhs: 11 12\n"
     ]
    }
   ],
   "source": [
    "##########数据统计和噪声处理#############\n",
    "label_dict = {\"neutral\":0,\"contradiction\":1,\"entailment\":2}\n",
    "vocab_set = set()\n",
    "lhs = []\n",
    "rhs = []\n",
    "for k,v in enumerate(train.sentence1):\n",
    "    if type(v) == float:\n",
    "        print(k,v)\n",
    "        invalid_idx.append(k)\n",
    "    else:\n",
    "        v = v.split()\n",
    "        lhs.append(v)\n",
    "        for j in v:\n",
    "            vocab_set.add(j)\n",
    "invalid_idx = []\n",
    "for k,v in enumerate(train.sentence2):\n",
    "    if type(v) == float:\n",
    "        print(k,v)\n",
    "        invalid_idx.append(k)\n",
    "    else:\n",
    "        v = v.split()\n",
    "        rhs.append(v)\n",
    "        for j in v:\n",
    "            vocab_set.add(j)\n",
    "for k,v in enumerate(train.gold_label):\n",
    "    if v not in label_dict.keys():\n",
    "        invalid_idx.append(k)\n",
    "\n",
    "print(\"词数：\",len(vocab_set))\n",
    "lhs.sort()\n",
    "rhs.sort()\n",
    "print(\"length of lhs and rhs:\",len(lhs[int(0.5*len(lhs))]),len(rhs[int(0.5*len(rhs))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理输入格式\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x1train = []\n",
    "x2train = []\n",
    "ytrain = []\n",
    "for k,v in enumerate(train.sentence1):\n",
    "    if k not in invalid_idx:\n",
    "        x1train.append(v)\n",
    "for k,v in enumerate(train.sentence2):\n",
    "    if k not in invalid_idx:\n",
    "        x2train.append(v)\n",
    "for k,v in enumerate(train.gold_label):\n",
    "    if k not in invalid_idx:\n",
    "        ytrain.append(v)\n",
    "x1train = pd.Series(x1train)\n",
    "x2train = pd.Series(x2train)\n",
    "ytrain = pd.Series(ytrain)\n",
    "x1test = test.sentence1\n",
    "x2test = test.sentence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(x1train.values)\n",
    "x1_train = tk.texts_to_sequences(x1train)\n",
    "x2_train = tk.texts_to_sequences(x2train)\n",
    "x1_test = tk.texts_to_sequences(x1test)\n",
    "x2_test = tk.texts_to_sequences(x2test)\n",
    "x1_train = pad_sequences(x1_train, lsen_len)\n",
    "x2_train = pad_sequences(x2_train, rsen_len)\n",
    "x1_test = pad_sequences(x1_test, lsen_len)\n",
    "x2_test = pad_sequences(x2_test, rsen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "########验证集数据处理##########\n",
    "dev_invalid_idx = []\n",
    "x1dev = []\n",
    "x2dev = []\n",
    "ydev = []\n",
    "for k,v in enumerate(dev.sentence1):\n",
    "    if type(v) == float:\n",
    "        print(k,v)\n",
    "        dev_invalid_idx.append(k)\n",
    "for k,v in enumerate(dev.sentence2):\n",
    "    if type(v) == float:\n",
    "        print(k,v)\n",
    "        dev_invalid_idx.append(k)\n",
    "for k,v in enumerate(dev.gold_label):\n",
    "    if v not in label_dict.keys():\n",
    "        #print(k,v)\n",
    "        dev_invalid_idx.append(k)\n",
    "\n",
    "for k,v in enumerate(dev.sentence1):\n",
    "    if k not in dev_invalid_idx:\n",
    "        x1dev.append(v)\n",
    "for k,v in enumerate(dev.sentence2):\n",
    "    if k not in dev_invalid_idx:\n",
    "        x2dev.append(v)\n",
    "for k,v in enumerate(dev.gold_label):\n",
    "    if k not in dev_invalid_idx:\n",
    "        ydev.append(v)\n",
    "x1_dev = tk.texts_to_sequences(x1dev)\n",
    "x2_dev = tk.texts_to_sequences(x2dev)\n",
    "x1_dev = pad_sequences(x1_dev, lsen_len)\n",
    "x2_dev = pad_sequences(x2_dev, rsen_len)\n",
    "y_dev = []\n",
    "count = 0\n",
    "for i in ydev:\n",
    "    y_dev.append(label_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ESIM Model##############################################\n",
    "def build_model(embedding_matrix=None):\n",
    "    ############输入编码##############\n",
    "    q1_inputs = Input(name='q1',shape=(lsen_len,))\n",
    "    q2_inputs = Input(name='q2',shape=(rsen_len,))#这里两个输入的长度不一定要保持一致，因为我们attention的是词和词\n",
    "    if embedding_matrix is None:\n",
    "        embedding = Embedding(vocab_size, embedding_size)\n",
    "    else:\n",
    "        embedding = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)\n",
    "    bn = BatchNormalization(axis=2)\n",
    "    q1_embed = bn(embedding(q1_inputs))\n",
    "    q2_embed = bn(embedding(q2_inputs))\n",
    "    q1_encoded = Bidirectional(LSTM(lstm_dim//2, return_sequences=True))(q1_embed)\n",
    "    q2_encoded = Bidirectional(LSTM(lstm_dim//2, return_sequences=True))(q2_embed)\n",
    "    #print(\"q1,q2 shape after bilstm:\",q1_encode.shape,q2_encode.shape)#输出格式为(x1,x2,x3),x1是批数，x2是句子长度，x3是bilstm隐层变量\n",
    "    ############局部推理(attention)##############\n",
    "    def multiply(tensor):\n",
    "        input_1 = tensor[0]\n",
    "        input_2 = tensor[1]\n",
    "        #in1_aligned = K.zeros([input_1.shape[0], input_1.shape[2]], dtype=tf.float32)\n",
    "        for i in range(lsen_len):\n",
    "            attention_1 = Dot(axes=-1)([input_1[:,i,:],input_2])#(bs,8)\n",
    "            w_att_1 = Lambda(lambda x: keras.activations.softmax(x, axis=1),)(attention_1)#(bs,8)\n",
    "            vector = Dot(axes=1)([w_att_1,input_2])#(bsx8)和(bsx8x256)作点乘，得到(bsx256)\n",
    "            att_value = K.expand_dims(vector,axis=1)#把维度变成bsx1x256\n",
    "            if i == 0:\n",
    "                in1_aligned = att_value\n",
    "            else:\n",
    "                in1_aligned = tf.concat([in1_aligned,att_value],1)\n",
    "        #print(\"shape of my heart1:\",attention_1.shape,w_att_1.shape,vector.shape,att_value.shape,in1_aligned.shape)\n",
    "\n",
    "        #in2_aligned = K.zeros([input_1.shape[0], input_1.shape[2]], dtype=tf.float32)\n",
    "        for i in range(rsen_len):\n",
    "            attention_2 = Dot(axes=-1)([input_1,input_2[:,i,:]])#(bs,14)\n",
    "            w_att_2 = Lambda(lambda x: keras.activations.softmax(x, axis=1),)(attention_2)#(bs,14)\n",
    "            vector = Dot(axes=1)([w_att_2,input_1])#(bsx14)和(bsx14x256)作点乘，得到(bsx256)\n",
    "            att_value = K.expand_dims(vector,axis=1)#把维度变成bsx1x256\n",
    "            if i == 0:\n",
    "                in2_aligned = att_value\n",
    "            else:\n",
    "                in2_aligned = tf.concat([in2_aligned,att_value],1)\n",
    "        #print(\"shape of my heart2:\",attention_2.shape,w_att_2.shape,vector.shape,att_value.shape,in2_aligned.shape)\n",
    "        #print(type(in1_aligned),type(in2_aligned))\n",
    "\n",
    "        return [in1_aligned,in2_aligned]#这里如果不是return一个list会报错\n",
    "    \n",
    "    [in1_aligned, in2_aligned]=Lambda(multiply)([q1_encoded,q2_encoded])\n",
    "    ############推理合成##############\n",
    "    #print(\"shape:\",in1_aligned.shape,in2_aligned.shape)\n",
    "    # Compare\n",
    "    print(K.tf.multiply(q1_encoded, in1_aligned))\n",
    "    q1_combined = concatenate(\n",
    "        [q1_encoded, in1_aligned, subtract([q1_encoded, in1_aligned]), Lambda(lambda x:K.tf.multiply(x[0],x[1]))([q1_encoded, in1_aligned])])\n",
    "    q2_combined = concatenate(\n",
    "        [q2_encoded, in2_aligned, subtract([q2_encoded, in2_aligned]), Lambda(lambda x:K.tf.multiply(x[0],x[1]))([q2_encoded, in2_aligned])])\n",
    "    q1_dense1 = Dense(compare_dim, activation='relu')(q1_combined)\n",
    "    q2_dense1 = Dense(compare_dim, activation='relu')(q2_combined)\n",
    "    q1_compare = Dropout(compare_dropout)(q1_dense1)\n",
    "    q2_compare = Dropout(compare_dropout)(q2_dense1)\n",
    "#     dense2 = Dense(compare_dim, activation='relu')(drop1)\n",
    "#     drop2 = Dropout(compare_dropout)(dense2)\n",
    "    #q1_compare = drop2(q1_combined)\n",
    "    #q2_compare = drop2(q2_combined)\n",
    "\n",
    "    # Aggregate\n",
    "    q1_maxp = MaxPooling1D()(q1_compare)\n",
    "    q1_avgp = AveragePooling1D()(q1_compare)\n",
    "    q2_maxp = MaxPooling1D()(q2_compare)\n",
    "    q2_avgp = AveragePooling1D()(q2_compare)\n",
    "    res = Concatenate(1)([q1_maxp,q1_avgp,q2_maxp,q2_avgp])\n",
    "    flat = Flatten()(res)\n",
    "    dense3 = Dense(128, activation='relu')(flat)\n",
    "    preds = Dense(3, activation='softmax')(dense3)\n",
    "    \n",
    "    model = Model(inputs=[q1_inputs,q2_inputs], outputs=[preds])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul:0\", shape=(?, 14, 256), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "q1 (InputLayer)                 (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q2 (InputLayer)                 (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             18910500    q1[0][0]                         \n",
      "                                                                 q2[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor multiple             1200        embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 14, 256)      439296      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 8, 256)       439296      batch_normalization_1[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               [(None, 14, 256), (N 0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 14, 256)      0           bidirectional_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_46 (Lambda)              (None, 14, 256)      0           bidirectional_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 8, 256)       0           bidirectional_2[0][0]            \n",
      "                                                                 lambda_1[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_47 (Lambda)              (None, 8, 256)       0           bidirectional_2[0][0]            \n",
      "                                                                 lambda_1[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 14, 1024)     0           bidirectional_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 subtract_1[0][0]                 \n",
      "                                                                 lambda_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 1024)      0           bidirectional_2[0][0]            \n",
      "                                                                 lambda_1[0][1]                   \n",
      "                                                                 subtract_2[0][0]                 \n",
      "                                                                 lambda_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 14, 128)      131200      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8, 128)       131200      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 14, 128)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 8, 128)       0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 7, 128)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_1 (AveragePoo (None, 7, 128)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 4, 128)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_2 (AveragePoo (None, 4, 128)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 22, 128)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 average_pooling1d_1[0][0]        \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 average_pooling1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2816)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          360576      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            387         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 20,413,655\n",
      "Trainable params: 20,413,055\n",
      "Non-trainable params: 600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "549361/549361 [==============================] - 7282s 13ms/step - loss: 0.7560 - acc: 0.6697\n",
      "Epoch 2/7\n",
      "549361/549361 [==============================] - 7073s 13ms/step - loss: 0.6354 - acc: 0.7361\n",
      "Epoch 3/7\n",
      "549361/549361 [==============================] - 7346s 13ms/step - loss: 0.5751 - acc: 0.7663\n",
      "Epoch 4/7\n",
      "549361/549361 [==============================] - 7571s 14ms/step - loss: 0.5287 - acc: 0.7890\n",
      "Epoch 5/7\n",
      "549361/549361 [==============================] - 7127s 13ms/step - loss: 0.4879 - acc: 0.8071\n",
      "Epoch 6/7\n",
      "549361/549361 [==============================] - 7498s 14ms/step - loss: 0.4495 - acc: 0.8244\n",
      "Epoch 7/7\n",
      "549361/549361 [==============================] - 8009s 15ms/step - loss: 0.4165 - acc: 0.8384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14498024080>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1_train,x2_train], y_train, batch_size=32, epochs=7, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9842/9842 [==============================] - 8s 811us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.601235988493301, 0.7725055883192866]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x1_dev,x2_dev], y_dev, batch_size=32, verbose=1)#loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最后结果比paper提出的差不少，猜测原因：epoch太少，没有用预先的word embedding(paper中是Glove)，以及推理合成层那里稍有不同。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
